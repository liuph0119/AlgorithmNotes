# PART 3 超参数调试、Batch正则化和程序框架

## 1. 调试处理

比较重要的需要调试的参数主要包括 

- 最重要：学习率$\alpha$ 
- 次要：惯性量$\beta$、 隐层单元数目、 mini-batch size
- 其次：隐层数目、学习率衰减系数
- 一般无需修改：$\beta_1$、 $\beta_2$、 $\epsilon$

不要采用网格搜寻进行参数寻优，由于不知道哪些参数是比较重要和敏感的，推荐采用`随机搜索`的方式，先确定几个效果较好的点，接着在该范围内进行精细地搜索。

## 2. 为超参数选择合适的范围

以学习率$\alpha$为例，假设其取值范围为0.0001~1，采用标准的随机取值效果不一定好，因为绝大部分随机数集中在0.1~1。因此可以考虑`以log标尺随机取值`。

```python
r = -4 * np.random.rand()	# r ∈ [-4, 0]
alpha = 10**r				# α ∈ [10^-4, 10^0]
```

以$\beta$为例，假设其取值范围为0.9~0.999。可以考虑为$1-\beta$确定值，进而确定$\beta$。$1-\beta$范围为0.001~0.1，采用同上方式随机取值。

## 3. 超参数调试的实践

- 细心微调一个model
- 并行计算多个model，挑选表现优良的那些

## 4. Batch Norm（浅略）

归一化输入层可以加快训练，归一化隐含层的激活值也可加快后面层的训练速度：

$\mu = \frac{1}{m}\sum_i^mz^{(i)}$

$\sigma^2 = \frac{1}{m}\sum_i^m(z^{(i)}-\mu)^2$

$z_{norm}^{(i)} = \frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}$

$\widetilde{z}^{(i)} = \gamma z_{norm}^{(i)}+\beta$，if $\gamma = \sqrt{\sigma^2+\epsilon}$，$\beta = \mu$， then $\widetilde{z}^{(i)} =   z^{(i)}$

这里 $\gamma$和$\beta$是需要学习的超参数。

## 5. Softmax 回归

$\hat{y} = \mathbf{p}$，其中$\mathbf{p}$为各类别的概率组成的向量，且$\sum_i^Cp_i=1$

对于含有$n_l​$个节点的layer L，其激活值为$\mathbf{a}​$（$n_l​$个元素的向量），如果其后接一层softmax激活函数直接输出y，那么首先计算其指数值$\mathbf{t} = e^\mathbf{a}​$，计算$\mathbf{t}​$的求和值$t_{sum}​$，最后输出$\mathbf{p} = \frac{ \mathbf{t}}{t_{sum}}​$。

softmax实际上是logistic的多类扩展，其仍然是一个线性的分类器。

## 6. Deep Learning Frameworks

- Caffe / Caffe2
- CNTK
- DL4J
- Keras
- Lasagne
- mxnet
- PaddlePaddle
- TensorFlow
- Theano
- Torch

