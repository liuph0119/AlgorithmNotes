# PART 1 实用层面

## 1. 数据集的划分(training/validate/testing set)

　　通常将数据划分为`training set`/`validate set`/`testing set`。在传统数据时代（数据量不多的情况下），一般来说如果只划分`training set`和`testing set`通常将其划分为`70%`和`30%`；如果划分为三部分，通常比例为`60%`,`20%`和`20%`。而在 big data 的时代，通常`validate set`/`testing set`只需要很小的部分，可能只需要`1%`，其余的`98%`全部用来做训练。对于百万级别的数据量，训练集可以占到`99.5%`，其余两部分只需要占到`20%~10%`以下。

- ps：要确保训练集和验证集数据来自同一分布。

## 2. 偏差&方差（Bias & Variance）
- `high variance`: 过拟合（训练集的误差很低，验证集的误差很高）
- `high bias`: 欠拟合（训练集的误差和验证集的误差都很高）
- `high bias & high variance`: 训练集的误差很高，验证集的误差更高
- `low bias & low variance`: 训练集和验证集的误差都很低

一般来说，线性分类器比较容易欠拟合，而曲线和高次函数容易过拟合，因为其对部分outlier样本过度拟合。

## 3. 机器学习基础

　　要想了解算法是都适用，首先需要评估其bias，如果bias很大，那么可以考虑`新的网络结构（隐层更多或者隐层单元更多等）、训练更长时间、更好的优化算法` ；一旦bias降低，接下来评估variance，如果viriance很高，可以考虑`使用更多的数据、正则化、更改网络框架`等方法。

`bias-variance trade-off`

## 4. 正则化 regularization

　　在此之前，先来来了解L1范数和L2范数：

- L2范数 $||w||_2^2$ = $\sum_{j=1}^{n_x}w_j^2$ = $w^{T}w$，其中$n_x$为$\vec{w}$的维度。
- L1范数 $||w||_{1}$ = $\sum_{j=}1^{n_x}||w||$。

　　因此，在训练过程中，正则化项：

- L2正则化：$J$ = $J$ + $\frac{\lambda}{2m}||w^{[l]}||_2^2$，是目前最常使用的正则项，又被称为“权重衰减”。
- L1正则化：$J$ = $J$ + $\frac{\lambda}{2m}||w^{[l]}||_1$，L1正则化会使得$w$稀疏化，即其中很多$w_j$值为0。
- 其中$\lambda$为正则化参数， $l$为权重的序号，在NN中，$l$可以代表网络层，即$W^{[l]}$可以表示第$l$层的网络权重。

　　在训练神经网络模型时，如果加入正则项，那么在计算梯度更新网络权重时，需要在梯度后面加上正则项的偏导，即$\frac{\partial J}{\partial W^{[l]}}$ =$\frac{\partial J}{\partial W^{[l]}}$ +$\frac{\lambda}{m}W^{[l]}$ ，之后再$W^{[l]} = W^{[l]} - \alpha \frac{\partial J}{\partial W^{[l]}}$=$(1-\frac{\alpha \lambda}{m})W^{[l]} - \alpha \frac{\partial J}{\partial W{[l]}}$

`那么，为什么正则化可以避免过拟合呢？`

　　其实质是实现了一种从过拟合到欠拟合的过渡状态，即加入正则化之后，大部分权重设为0，整个深层神经网络的许多节点之间的权重为0，`神经网络结构（通常会过拟合）会趋向于“修剪”到等价于一个深层的逻辑回归模型（通常会欠拟合）`，而$\lambda$使得其达到一个这种状态，这种状态对数据的拟合效果通常恰好。

　　其次，如果Ｗ的值相对较小，那么f(X) = WX+b的值也会较小，因此在采用激活函数之前，其值接近0。对于tanh和sigmoid等激活函数，`当z取值接近0时通常为线性`。当z与0偏离越大，非线性也越明显。当W较小时，`整个网络也就更线性，更容易避免过拟合`。

## 5. 正则化 dropout

　　对于每一次采用一部分样本进行训练，都对每个节点设定一个概率阈值`keep_prob `，以`1-keep_prob `的概率删除该节点，并且删除其相连接的节点权重，使得网络结果精简化。对于每次样本训练都进行一次dropout。测试阶段不进行dropout。

　　以`Inverted Dropout`方法为例，对于一个3层的神经网络，

```python
d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob
a3 = np.multiply(a3, d3)
a3 /= keep_prob	#因为有1-keep_prob的节点被删除，因此a3值的整体期望减少了(1-keep_prob)，因此需要除以keep_prob使得a4的期望不变。
```

　　在进行dropout正则化时，主要需要调整参数keep_prob，其决定了该层神经网络节点保留下来的概率，对于`可能出现过拟合且参数较多的层，可以将keep_prob设为一个较小的值`，以便应用更强大的dropout。对于`输入层和不易于过拟合的层，可以将keep_prob设为1或者较大的值`。

　　dropout的一大缺点是 cost function 的定义不再明确定义，因此在训练过程中，可以先设定keep_prob为1，之后再打开dropout。

## 6. 其他正则化方法

- 数据扩增 data augmentation

  ​	对图片进行翻转、旋转、裁剪

- 提早停止迭代 early stopping

  ​	验证集上的误差不再下降反而升高之前停止训练。（神经网络的权重会随着训练次数的增加而激增，为了避免权重过大过拟合）

## 7. 标准化输入 Normalizing training sets

​	$\mu = \frac{1}{m}\sum_{i=1}^{m}x_{(i)}$

​	$X = X-\mu$

​	$\sigma^2 = \frac{1}{m}\sum_{i=1}^{m}x_{(i)}^2$

​	$X /= \sigma^2$

　　训练集和测试集需要采用相同的$\mu$和$\sigma^2$来进行归一化处理。

　　如果未进行归一化处理，$J$不均匀，当初始位置位于较远的位置时，需要经历多次迭代才能到达局部最优的位置；而进行标准化处理之后损失函数更加均匀，无论初始位置在哪儿都可以经过较少的迭代次数就到达局部最优，从而加快训练速度。

## 8. 梯度消失和梯度爆炸 Vanishing/exploding gradients

　　在训练一个极深的神经网络时，以线性激活函数g(z)=z为例，如果权重大于1，激活值和偏导数将以指数递增，反之以指数递减。

## 9. 权重初始化

​		$W^{[l]} = np.random.randn(W^{[l]}.shape)*var_{w^{[l]}}$

- 通常，$var_{w^{[l]}}$取值为$\frac{1}{n^{[n-1]}}$对于


- tanh激活函数，可以采用Xavier 初始化，即$var_{w^{[l]}}$取值为$\sqrt{\frac{1}{n^{[n-1]}}}$， 也可以取值为$\sqrt{\frac{2}{n^{[n-1]} + n^{[l]}}}$


- 对于Relu激活函数，$var_{w^{[l]}}$取值为$\sqrt{\frac{2}{n^{[n-1]}}}$。


采用适当的权重，可以避免其值过快下降到0，也可避免值增长过快，一定程度上避免梯度消失和梯度爆炸的问题。

## 10. 梯度检验

$\frac{f(\theta+\epsilon) - f(\theta-\epsilon)}{2\epsilon}\approx g(\theta)$，采用这种双边误差计算梯度，误差一般为$O(\epsilon^2)$

进行梯度检验时，将所有权重参数转为１列向量并且链接为一个大的向量$\mathbf{\theta}$，之后对$\mathbf{\theta}$中的每个参数$\mathbf{\theta}_i$，采用双边误差计算梯度$d\theta_i$。

之后对数值逼近求得的梯度$d\theta_{approx}$和偏导求得的梯度$d\theta$计算距离 $\frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{spprox}||_2+||d\theta||_2}$，如果距离小于$10^{-7}$，那么认为梯度检验没问题。


