# PART 2 优化算法

## 1. Mini-batch gradient descent

采用全部样本进行训练的方法称为batch，但这种方法在数据样本非常多时计算量巨大，速度慢。mini-batch将一定数目的样本作为一个batch，进行训练，从而比batch梯度下降法运行更快。

采用batch梯度下降进行训练时，损失函数$J$应该会随着每次迭代而不断减小。采用mini-batch gradient descent时不一定每一次迭代的误差都是下降的，会有较大的波动，但是整体误差是降低的。

$m$为训练集的样本数目，极端情况下mini-batch值为$m$或者1：

- mini-batch = $m$：称为batch gradient descent。单词迭代耗时较长，噪声低，下降幅度大。适用于样本集较少的情况。


- mini-batch = 1：称为stochastic gradient descent。失去了向量化带来的加速处理，噪声较大，永远不会收敛。
- mini-batch介于1和$m$之间：向量化加速，噪声较小。一般取值64~512，采用2的次方会有效利用电脑内存。

## 2. Exponentially weighted averages

$V_t = \beta V_{t-1} + (1-\beta){\theta}_t$， $\beta$取值较高时，曲线更平滑一些，且偏右移。实际上 $V_t = \sum_{i=0}^{t-1}(1-\beta)\beta^i\theta_{t-i}$。

取值$V_0 = 0$，进而计算$V_1,...,V_t$时，由于初始值偏低（原本为$\beta$，结果成为$\beta(1-\beta)$），因此有必要对其进行bias correction。即，在每次计算时，用$\frac{V_t}{1-\beta^t}$代替$V_t$，$V_t = \frac{\beta V_{t-1} + (1-\beta){\theta}_t}{1-\beta^t}$。

## 3. Gradient descent with momentum

- 初始化 $v_{dW}=\mathbf0$，$v_{db}=\mathbf0$ 
- For each iteration $t$，采用mini-batch计算$dW$和$db$；
- $v_{dW} = \beta v_{dW} + (1-\beta)dW$，  $v_{db} = \beta v_{db} + (1-\beta)db$ （$\beta$一般可以取值0.9）


- $W = W - \alpha v_{dW}$， $b = b - \alpha _v{db}$
- 当迭代次数小于$T$次时收到bias的影响，需要将$v_dw$和$v_db$除以$(1-\beta^t)$；而当$t$ > $T$时，不再收到偏差的影响，不用再除

## 4. RMSprop

- $S_{dW}=0$，$S_{db}=0$
- For each iteration $t$，采用mini-batch计算$dW$和$db$；
- $S_{dW} = \beta S_{dW} + (1-\beta)(dW)^2$，$S_{db} = \beta S_{db} + (1-\beta)(db)^2$
- $W = W - \alpha\frac{dW}{\sqrt{S_{dW}}+\epsilon}$，$b = b - \alpha\frac{db}{\sqrt{S_{db}}+\epsilon}$

## 5. Adam

Adam实质上是将RMSprop和Momentum结合起来。

- 初始化 $v_{dW}=\mathbf0$，$v_{db}=\mathbf0$ ，$S_{dW}=0$，$S_{db}=0$
- For each iteration $t$，采用mini-batch计算$dW$和$db$；
- $v_{dW} = \beta_1 v_{dW} + (1-\beta_1)dW$，  $v_{db} = \beta_1 v_{db} + (1-\beta_1)db$ ，$S_{dW} = \beta_2 S_{dW} + (1-\beta_2)(dW)^2$，$S_{db} = \beta_2 S_{db} + (1-\beta_2)(db)^2$
- $v_{dW}^{corrected} = \frac{v_{dW}}{1-\beta_1^2}$，$v_{dW}^{corrected}$ = $\frac{v_{db}}{1-\beta_1^2}$，$S_{dW}^{corrected}$ = $\frac{S_{dW}}{1-\beta_2^2}$，$S_{db}^{corrected}$ = $\frac{S_{db}}{1-\beta_2^2}$
- $W = W - \alpha\frac{v_{dw}^{corrected}}{\sqrt{S_{dW}^{corrected}+\epsilon}}$，$b = b - \alpha\frac{v_{db}^{corrected}}{\sqrt{S_{db}^{corrected}+\epsilon}}$
  ps: $\alpha$为学习率，需要微调；$\beta_1$一般取值0.9，$\beta_2$一般取值0.999，$\epsilon$一般取值$10^{-8}$，无需调整。

## 6. 学习率衰减

1. $\alpha =  \frac{1}{1+DecayRate\times EpochNum }\alpha_0$
2. $\alpha = 0.95^{EpochNum} \cdot \alpha_0$
3. $\alpha =  \frac{k}{EpochNum}\cdot\alpha_0$
4. discrete learning rate...



部分优化算法的C++实现（亲测有效）：

```C++
#ifndef SAE_OPTIMIZER_H
#define SAE_OPTIMIZER_H

#include "matrix.h"
#include "preprocess.h"
#include <math.h>
#include <algorithm>
#include <string>
#include <vector>


// SGD, Adagrad的初始权重需要较大;
class Optimizer {
public:
	Optimizer(double _lr = 0.01, double _decay = 0) { learning_rate = _lr; dDecay = _decay; nIterations = 0; }
	virtual ~Optimizer() {}

	virtual void update_weights(Matrix &W, Matrix &b, Matrix dW, Matrix db) {}

protected:
	double learning_rate;
	double dDecay;
	int nIterations;
};

class SGD :public Optimizer
{
public:
	static std::string name() { return "SGD"; }
	SGD(double _lr = 0.01, double _decay = 0)
	{
		learning_rate = _lr;
		dDecay = _decay;
		nIterations = 0;
	}
	~SGD(){}
	virtual void update_weights(Matrix &W, Matrix &b, const Matrix dW, const Matrix db)
	{
		double _lr = learning_rate;
		if (dDecay > 0)
			_lr = 1.0 / (1.0 + nIterations*dDecay)*learning_rate;

		W = W - _lr * dW;
		b = b - _lr * db;

		nIterations++;
	}
};


class Adagrad :public Optimizer
{
private:
	double dSigma_1, dSigma_2;
public:
	static std::string name() { return "Adagrad"; }
	Adagrad(double _lr = 0.5, double _decay = 0)
	{
		learning_rate = _lr;
		dDecay = _decay;
		nIterations = 0;
	}
	~Adagrad() {}
	virtual void update_weights(Matrix &W, Matrix &b, Matrix dW, Matrix db)
	{
		if (nIterations == 0)
		{
			dSigma_1 = 0;
			dSigma_2 = 0;
		}

		double _lr = learning_rate;
		if (dDecay > 0)
			_lr = 1.0 / (1.0 + nIterations*dDecay)*learning_rate;
		

		dSigma_1 += sumMat(powMat(dW, 2));
		dSigma_2 += sumMat(powMat(db, 2));

		W = W - _lr / std::sqrt(nIterations + 1.0) / std::sqrt(dSigma_1 / (nIterations + 1)) * dW;
		b = b - _lr / std::sqrt(nIterations + 1.0) / std::sqrt(dSigma_2 / (nIterations + 1)) * db;
		
		nIterations++ ;
	}
};

class Momentum :public Optimizer
{
private:
	double beta;
	int nT;
	Matrix vdW, vdb;
public:
	static std::string name() { return "Momentum"; }
	Momentum(double _lr = 0.05, double _beta = 0.9, int _nT = 100, double _decay = 0) {
		learning_rate = _lr; 
		beta = _beta; 
		nT = _nT; 
		dDecay = _decay;
		nIterations = 0;
	}
	~Momentum() {  }
	virtual void update_weights(Matrix &W, Matrix &b, const Matrix dW, const Matrix db)
	{
		if (nIterations == 0)
		{
			vdW = Matrix(dW.RowNo(), dW.ColNo());
			vdb = Matrix(db.RowNo(), db.ColNo());
			vdW.Null();
			vdb.Null();
		}

		double _lr = learning_rate;
		if (dDecay > 0)
			_lr = 1.0 / (1.0 + nIterations*dDecay)*learning_rate;

		vdW = beta*vdW + (1 - beta)*dW;
		vdb = beta*vdb + (1 - beta)*db;


		if (nIterations < nT)
		{
			W = W - _lr / (1 - beta*beta) * vdW;
			b = b - _lr / (1 - beta*beta) * vdb;
		}
		else
		{
			W = W - _lr * vdW;
			b = b - _lr * vdb;
		}
		nIterations++;
	}
};


class RMSprop :public Optimizer
{
private:
	double beta;
	double dEpsilon;
	Matrix SdW, Sdb;
public:
	static std::string name() { return "RMSprop";}
	RMSprop(double _lr = 0.1, double _beta = 0.999, double _decay = 0) :dEpsilon(10e-8)
	{
		learning_rate = _lr;
		beta = _beta;
		dDecay = _decay;
		nIterations = 0;
	}
	~RMSprop() {}
	virtual void update_weights(Matrix &W, Matrix &b, const Matrix dW, const Matrix db)
	{
		if (nIterations == 0)
		{
			SdW = Matrix(dW.RowNo(), dW.ColNo());
			Sdb = Matrix(db.RowNo(), db.ColNo());
			SdW.Null();
			Sdb.Null();
		}

		double _lr = learning_rate;
		if (dDecay > 0)
			_lr = 1.0 / (1.0 + nIterations*dDecay)*learning_rate;
		
		SdW = beta*SdW + (1.0 - beta)*(powMat(dW, 2.0));
		Sdb = beta*Sdb + (1.0 - beta)*(powMat(db, 2.0));


		W = W - _lr * (devisionMat(dW, plusMat(sqrtMat(SdW), dEpsilon)));
		b = b - _lr * (devisionMat(db, plusMat(sqrtMat(Sdb), dEpsilon)));

		nIterations++;
	}
};


class Adam: public Optimizer
{
private:
	const double beta_1, beta_2;
	double dEpsilon;
	Matrix vdW, vdb, SdW, Sdb;

public:
	static const std::string name() { return "Adam"; }
	Adam(double _lr = 0.05, double _decay = 0) : beta_1(0.9), beta_2(0.999), dEpsilon(10e-8), Optimizer() { learning_rate = _lr; 
		dDecay = _decay; 
		nIterations = 0;
	}
	virtual ~Adam() {}

	virtual void update_weights(Matrix &W, Matrix &b, const Matrix dW, const Matrix db)
	{
		if (nIterations == 0)
		{
			vdW = Matrix(dW.RowNo(), dW.ColNo());
			vdb = Matrix(db.RowNo(), db.ColNo());
			SdW = Matrix(dW.RowNo(), dW.ColNo());
			Sdb = Matrix(db.RowNo(), db.ColNo());
			vdW.Null();
			vdb.Null();
			SdW.Null();
			Sdb.Null();
		}

		double _lr = learning_rate;
		if (dDecay > 0)
			_lr = 1.0 / (1.0 + nIterations*dDecay)*learning_rate;

		vdW = beta_1*vdW + (1.0 - beta_1)*dW;
		vdb = beta_1*vdb + (1.0 - beta_1)*db;
		SdW = beta_2*SdW + (1.0 - beta_2)*(powMat(dW, 2.0));
		Sdb = beta_2*Sdb + (1.0 - beta_2)*(powMat(db, 2.0));

		Matrix vcdW =vdW / (1 - beta_1*beta_1),
			vcdb = vdb / (1 - beta_1*beta_1),
			ScdW = SdW / (1 - beta_2*beta_2),
			Scdb = Sdb /(1 - beta_2*beta_2);

		W = W - _lr * (devisionMat(vcdW, plusMat(sqrtMat(ScdW), dEpsilon)));
		b = b - _lr * (devisionMat(vcdb, plusMat(sqrtMat(Scdb), dEpsilon)));

		nIterations++;
	}
};


Optimizer* new_Optimizer(std::string sOptimizerType, double _lr = 0.01)
{
	if (sOptimizerType == "") return NULL;
	std::string act(sOptimizerType);
	if (act.compare(SGD::name()) == 0) return new SGD(_lr);
	if (act.compare(Momentum::name()) == 0) return new Momentum(_lr);
	if (act.compare(RMSprop::name()) == 0) return new RMSprop(_lr);
	if (act.compare(Adam::name()) == 0) return new Adam(_lr);
	if (act.compare(Adagrad::name()) == 0) return new Adagrad(_lr);
	
	return NULL;
}

#endif

```

